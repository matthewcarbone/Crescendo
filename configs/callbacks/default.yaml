defaults:
  - _self_

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint

  # directory to save the model file
  dirpath: ${paths.checkpoint_dir}

  # checkpoint filename
  # Can also consider something like "epoch_{epoch:03d}"
  # filename: "best"
  filename: "{epoch:06d}"

  # name of the logged metric which determines when model is improving
  monitor: "val/loss"

  # "max" means higher metric value is better, can be also "min"
  mode: "min"

  # verbosity mode
  verbose: True

  # additionally always save an exact copy of the last checkpoint to a file last.ckpt
  save_last: True

  # save k best models (determined by above metric)
  save_top_k: 1

  # when True, the checkpoints filenames will contain the metric name
  auto_insert_metric_name: True

  # if True, then only the modelâ€™s weights will be saved
  save_weights_only: False

  # number of training steps between checkpoints
  every_n_train_steps: null

  # checkpoints are monitored at the specified time interval
  train_time_interval: null

  # number of epochs between checkpoints
  every_n_epochs: null

  # whether to run checkpointing at the end of the training epoch or the end of validation
  save_on_train_epoch_end: null

early_stopping:
  _target_: lightning.pytorch.callbacks.EarlyStopping

  # quantity to be monitored, must be specified !!!
  monitor: "val/loss"

  # "max" means higher metric value is better, can be also "min"
  mode: "min"

  # minimum change in the monitored quantity to qualify as an improvement
  min_delta: 0.

  # number of checks with no improvement after which training will be stopped
  patience: 50

  # verbosity mode
  verbose: True

  # whether to crash the training if monitor is not found in the validation metrics
  strict: True

  # when set True, stops training when the monitor becomes NaN or infinite
  check_finite: True

  # stop training immediately once the monitored quantity reaches this threshold
  stopping_threshold: null

  # stop training as soon as the monitored quantity becomes worse than this threshold
  divergence_threshold: null

  # whether to run early stopping at the end of the training epoch
  check_on_train_epoch_end: null

model_summary:
  _target_: lightning.pytorch.callbacks.RichModelSummary

  # the maximum depth of layer nesting that the summary will include
  max_depth: 0
